{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.lstm import LSTM\n",
    "from model.rnn import RNN\n",
    "from model.lstm_teacher_forcing import LSTM_Forcing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param\n",
    "window_size = 10\n",
    "input_dim = 135\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "BNBUSDT= 0\n",
    "BTCUSDT= 1\n",
    "ETHUSDT= 2\n",
    "\n",
    "lookback = 20\n",
    "\n",
    "path_data = \"../training_data/data_norm_2021_2022.csv\"\n",
    "save_model_path = \"./results/\"\n",
    "# test_path = \"./data/data_2023.csv\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, path_data, window_size):\n",
    "        self.data = pd.read_csv(path_data, index_col=0)\n",
    "        self.seq_len  = window_size\n",
    "        self.index = self.data.index.unique()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index)-1\n",
    "    \n",
    "    def __getitem__(self, idx_time):\n",
    "        d_x = idx_time - self.seq_len + 1\n",
    "        d_y = idx_time\n",
    "        X = self.data.loc[self.index[d_x:idx_time]]\n",
    "        if X.empty:\n",
    "            X = np.zeros(((self.seq_len-1) *3, 5)).reshape(1,-1)\n",
    "            return X , np.array([0])\n",
    "        \n",
    "        data_y = self.data.loc[self.index[d_y:idx_time + 1]]\n",
    "        y = data_y[data_y[\"symbol\"]==BTCUSDT].close.values\n",
    "        \n",
    "        return np.array(X).reshape(1,-1), y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index < len(self.index):\n",
    "            result = self[self.current_index]\n",
    "            self.current_index += 1\n",
    "            return result\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrainingDataset(path_data, 10)\n",
    "data_loader = DataLoader(data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter(data)\n",
    "# next(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RNN(input_dim=135, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model_rnn.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# data_iter = iter(data_loader)\n",
    "# x_batch, y_batch = next(data_iter)\n",
    "# y_train_pred = model_rnn(x_batch.float())\n",
    "# loss = criterion(y_train_pred, y_batch.float())\n",
    "# optimiser.zero_grad()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # print(\"================3==============\")\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        # print(x_batch.shape)\n",
    "        # print(y_batch.shape)\n",
    "        y_train_pred = model_rnn(x_batch.float())\n",
    "        loss = criterion(y_train_pred, y_batch.float())\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Epoch \", epoch, \"MSE: \", loss.item())\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        # break\n",
    "\n",
    "    loss_values.append(total_loss)\n",
    "    if epoch%10==0:\n",
    "        # print(\"y_train_pred: \", y_train_pred, \"y_batch: \", y_batch)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {total_loss / len(data_loader)}\")\n",
    "        torch.save(model_rnn, \"results/model_rnn.pt\")\n",
    "    # break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # print(\"================3==============\")\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        # print(x_batch.shape)\n",
    "        # print(y_batch.shape)\n",
    "        y_train_pred = model(x_batch.float())\n",
    "        loss = criterion(y_train_pred, y_batch.float())\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Epoch \", epoch, \"MSE: \", loss.item())\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        # break\n",
    "    loss_values.append(total_loss /  len(data_loader))\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch {epoch}, Average Loss: {total_loss / len(data_loader)}\")\n",
    "    # break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM Teacher Forcing - Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class LSTM_Forcing(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(LSTM_Forcing, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "  def forward(self, input, future=0, y=None):\n",
    "    outputs = []\n",
    "\n",
    "    #reset the state of LSTM\n",
    "    #the state is kept till the end of the sequence\n",
    "    h_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "    c_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "    for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "      h_t, c_t = self.lstm(input_t, (h_t,c_t))\n",
    "      output = self.linear(h_t)\n",
    "      outputs += [output]\n",
    "\n",
    "    for i in range(future): #teacher forcing\n",
    "      if y is not None and random.random()>0.5:\n",
    "        output = y[:,[i]]\n",
    "      h_t, c_t = self.lstm(output,(h_t,c_t))\n",
    "      output = self.linear(h_t)\n",
    "      outputs += [output]\n",
    "    outputs = torch.stack(outputs,1).squeeze(2)\n",
    "    return outputs \n",
    "   \n",
    "model = LSTM_Forcing(input_size=input_dim, hidden_size=hidden_dim, output_size=output_dim)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter(data)\n",
    "x_batch, y_batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 135)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[266], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTMCell(\u001b[38;5;241m135\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1347\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[1;32m-> 1347\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1354\u001b[0m     ret \u001b[38;5;241m=\u001b[39m (ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), ret[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTMCell(135, 64)\n",
    "lstm(torch.from_numpy(x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# data_iter = iter(data_loader)\n",
    "# x_batch, y_batch = next(data_iter)\n",
    "# y_train_pred = model(x_batch)\n",
    "# loss = criterion(y_train_pred, y_batch.float())\n",
    "# optimiser.zero_grad()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # print(\"================3==============\")\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        # print(x_batch.shape)\n",
    "        # print(y_batch.shape)\n",
    "        y_train_pred = model(x_batch.float())\n",
    "        loss = criterion(y_train_pred, y_batch.float())\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Epoch \", epoch, \"MSE: \", loss.item())\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        # break\n",
    "    loss_values.append(total_loss /  len(data_loader))\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch {epoch}, Average Loss: {total_loss / len(data_loader)}\")\n",
    "    # break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
